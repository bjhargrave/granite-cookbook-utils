{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AY6igsZ2QTm"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, for utils development, install the local utils package in editable mode:\n",
    "\n",
    "`pip install -e ../. -U`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxEZGzJdGnJs"
   },
   "outputs": [],
   "source": [
    "!curl https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2aQog02Gouo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"nohup ollama serve &\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfrFV0gjGxni"
   },
   "outputs": [],
   "source": [
    "!ollama pull granite-code:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qTXzIbRN2hJ6",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ibm_granite_community.langchain_utils import find_langchain_model\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "replicate_model = find_langchain_model(platform=\"Replicate\", model_id= \"ibm-granite/granite-8b-code-instruct-128k\")\n",
    "ollama_model = find_langchain_model(platform=\"ollama\", model_id= \"granite-code:8b\")\n",
    "watsonx_model = find_langchain_model(platform=\"watsonx\", model_id= \"ibm/granite-13b-chat-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the models to complete a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2KQOBLBJk3x",
    "outputId": "16fd3cdb-ef29-453f-8165-a66fc6c23e37"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Complete this famous phrase: a shark on whisky is mighty risky, a shark on beer is\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Replicate: {replicate_model.invoke(prompt)}\")\n",
    "print(f\"Ollama: {ollama_model.invoke(prompt)}\")\n",
    "print(f\"WatsonX: {watsonx_model.invoke(prompt)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.langchain_utils import find_langchain_vector_db\n",
    "from langchain_core.embeddings import FakeEmbeddings\n",
    "embeddings_model = FakeEmbeddings(size=384)\n",
    "vector_db_provider = \"milvus\"\n",
    "db_file = \"/tmp/milvus.db\"\n",
    "\n",
    "vector_db = find_langchain_vector_db(vector_db_provider, embeddings_model, connection_args={\"uri\": db_file}, auto_id=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
